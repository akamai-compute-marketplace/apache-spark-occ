---
- name: install Java
  apt:
    name: openjdk-11-jdk
    state: present

- name: Download Spark with fallback
  block:
    - name: Try downloading Spark from main repository
      get_url:
        url: "https://downloads.apache.org/spark/{{ spark_version }}/{{ spark_version }}-bin-hadoop3.tgz"
        dest: "/tmp/{{ spark_version }}-bin-hadoop3.tgz"
  rescue:
    - name: Download Spark from archive repository
      get_url:
        url: "https://archive.apache.org/dist/spark/{{ spark_version }}/{{ spark_version }}-bin-hadoop3.tgz"
        dest: "/tmp/{{ spark_version }}-bin-hadoop3.tgz"

- name: extract spark
  unarchive:
    src: "/tmp/spark-3.5.3-bin-hadoop3.tgz"
    dest: "{{ spark_home }}"
    owner: spark
    group: spark
    remote_src: yes

- name: move spark download to spark home directory
  shell:
    cmd: mv /opt/spark/spark-{{ spark_version }}-bin-hadoop3/* /opt/spark

- name: Clean up downloaded archive
  file:
    path: "/tmp/spark-{{ spark_version }}-bin-hadoop3.tgz"
    state: absent

- name: remove /opt/spark/spark-{{ spark_version }}-bin-hadoop3
  file:
    path: "/opt/spark/spark-{{ spark_version }}-bin-hadoop3.tgz"\
    state: absent

- name: create spark configuration directory
  file:
    path: "{{ spark_home }}/conf"
    state: directory
    mode: '0755'

- name: Set ownership of Spark installation directory
  file:
    path: "{{ spark_home }}"
    owner: spark
    group: spark
    recurse: yes

- name: configure spark-env.sh on master
  template:
    src: templates/spark-env-master.sh.j2
    dest: "/opt/spark/conf/spark-env.sh"
    mode: '0755'
  delegate_to: localhost
  run_once: true

- name: configure spark-env.sh on workers
  template:
    src: templates/spark-env-worker.sh.j2
    dest: "/opt/spark/conf/spark-env.sh"
    mode: '0755'
  delegate_to: "{{ item }}"
  loop: "{{ groups['spark_workers'] }}"

- name: configure spark-defaults.conf on master
  template:
    src: templates/spark-defaults-master.conf.j2
    dest: "/opt/spark/conf/spark-defaults.conf"
    mode: '0755'
  delegate_to: localhost
  run_once: true

- name: configure spark-defaults.conf on workers
  template:
    src: templates/spark-defaults-worker.conf.j2
    dest: "/opt/spark/conf/spark-defaults.conf"
    mode: '0755'
  delegate_to: "{{ item }}"
  loop: "{{ groups['spark_workers'] }}"

- name: populate spark workers file
  template:
    src: templates/workers.j2
    dest: "/opt/spark/conf/workers"
    mode: '0644'

- name: move spark-master systemd service file
  ansible.builtin.copy:
    src: files/spark-master.service 
    dest: /etc/systemd/system/spark-master.service
    owner: spark
    group: spark
    mode: '0664'
  delegate_to: localhost
  run_once: true

- name: enable spark master
  systemd:
    name: spark-master
    enabled: yes
  delegate_to: localhost
  run_once: true

- name: move spark-workers systemd service file
  template:
    src: templates/spark-workers.service.j2
    dest: "/etc/systemd/system/spark-workers.service"
    owner: spark
    group: spark
    mode: '0644'
  delegate_to: localhost
  run_once: true

- name: enable spark workers
  systemd:
    name: spark-workers
    enabled: yes
  delegate_to: localhost
  run_once: true

- name: install scala
  apt:
    name: scala
    state: present
    update_cache: yes

- name: add spark environment variables to .bashrc
  lineinfile:
    path: "{{ ansible_env.HOME }}/.bashrc"
    create: yes
    line: "{{ item }}"
    state: present
  with_items:
    - 'export SPARK_HOME=/opt/spark'
    - 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin'

- name: source .bashrc
  shell: source ~/.bashrc
  args:
    executable: /bin/bash

- name: create spark-events directory
  file:
    path: /var/log/spark-events
    state: directory
    owner: root
    group: root
    mode: '0755'